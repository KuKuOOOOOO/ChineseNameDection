{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChineseDection2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KuKuOOOOOO/ChineseNameDection/blob/main/ChineseDection2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M6I3O73yo98"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysxkFoGWE9u1"
      },
      "source": [
        "img_url='https://github.com/KuKuOOOOOO/ChineseNameDection/archive/master.zip'\n",
        " \n",
        "import urllib\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "print('Downloading file...')\n",
        "urllib.request.urlretrieve(img_url,'ChineseNameDection-main.zip')\n",
        "print('Downloading Success')\n",
        "print('Extracting file...')\n",
        "with zipfile.ZipFile('ChineseNameDection-main.zip','r') as file:\n",
        "    file.extractall('')\n",
        "print('Done.')\n",
        "# 匯入相關所需的模組\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import h5py\n",
        "import glob\n",
        "import time\n",
        "from random import shuffle\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.utils import np_utils\n",
        "\n",
        "#字-Label encoding\n",
        "Chinese_data = {0:'HONG',1:'WEI',2:'FENG1',3:'FENG2',4:'XIANG',5:'LIAN',6:'NING',7:'HAO',8:'YU',9:'MING'}\n",
        "\n",
        "img_width = 32   #image長\n",
        "img_height = 32  #image寬\n",
        "num_class = len(Chinese_data) # 辨識字種類\n",
        "img_per_class = 1000 # 每個字將近500張訓練圖像\n",
        "test_size = 0.15 #測試分割%數\n",
        "\n",
        "#將訓練圖檔從檔案取出並轉成陣列\n",
        "def load_data():\n",
        "    path = './ChineseNameDection-main/'\n",
        "    images = []\n",
        "    labels = []\n",
        "    for k,v in Chinese_data.items(): #k:數字編碼,v:字label\n",
        "        # 把某一個字在檔案夾裡的所有圖像檔的路徑捉出來\n",
        "        image = [k for k in glob.glob(path + v + \"/*.jpg\")]\n",
        "        print(v + \":\" + str(len(image)))\n",
        "        for i,img in enumerate(image):\n",
        "            tmp_img = cv2.imread(img)\n",
        "            tmp_img = cv2.cvtColor(tmp_img,cv2.COLOR_BGR2GRAY)\n",
        "            ret,tmp_img=cv2.threshold(tmp_img,127,255,cv2.THRESH_BINARY)\n",
        "            tmp_img = cv2.resize(tmp_img,(img_height,img_width))\n",
        "            images.append(tmp_img)\n",
        "            labels.append(k)\n",
        "    data = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    return data, labels\n",
        "\n",
        "#取得訓練資料及驗證資料\n",
        "def get_dataset(save=False,load=False):\n",
        "    if load:\n",
        "        # 從檔案系統中載入之前處理保存的訓練資料集與驗證資料集\n",
        "        h5f = h5py.File('dataset.h5','r')\n",
        "        x_train = h5f['x_train'][:]\n",
        "        x_test = h5f['x_test'][:]\n",
        "        h5f.close()\n",
        "\n",
        "        # 從檔案系統中載入之前處理保存的訓練資料標籤與驗證資料集籤\n",
        "        h5f = h5py.File('labels.h5', 'r')\n",
        "        y_train = h5f['y_train'][:]\n",
        "        y_test = h5f['y_test'][:]\n",
        "        h5f.close()\n",
        "    else:\n",
        "        #從最原始圖檔開始整理\n",
        "        x,y = load_data()\n",
        "        y = keras.utils.np_utils.to_categorical(y,num_class) # 目標的類別種類數\n",
        "        # 將資料切分為訓練資料集與驗證資料集 (85% vs.  15%)\n",
        "        x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=test_size)\n",
        "        # 保存尚未進行歸一化的圖像數據\n",
        "        if save: \n",
        "            h5f = h5py.File('dataset.h5', 'w')\n",
        "            h5f.create_dataset('x_train', data=x_train)\n",
        "            h5f.create_dataset('x_test', data=x_test)\n",
        "            h5f.close()\n",
        "            \n",
        "            h5f = h5py.File('labels.h5', 'w')\n",
        "            h5f.create_dataset('y_train', data=y_train)\n",
        "            h5f.create_dataset('y_test', data=y_test)\n",
        "            h5f.close()\n",
        "    # 進行圖像每個像素值的型別轉換與歸一化處理\n",
        "    x_train = x_train.astype('float32') / 255.\n",
        "    x_test = x_test.astype('float32') / 255.\n",
        "    x_train = x_train.reshape(x_train.shape[0],img_width,img_height,1).astype('float32')\n",
        "    x_test = x_test.reshape(x_test.shape[0],img_width,img_height,1).astype('float32')\n",
        "    print(\"Train:\",x_train.shape,y_train.shape)\n",
        "    print(\"Test:\",x_test.shape,y_test.shape)\n",
        "\n",
        "    return x_train,x_test,y_train,y_test\n",
        "\n",
        "#建立Model\n",
        "def create_model_conv(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(128,(3, 3), padding='same',kernel_initializer='he_normal', activation='relu', input_shape=input_shape))\n",
        "    model.add(Conv2D(128,(3, 3), padding='same',kernel_initializer='he_normal', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(256,(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
        "    model.add(Conv2D(256,(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(512,(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
        "    model.add(Conv2D(512,(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
        "    model.add(Conv2D(512,(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, kernel_initializer='he_normal', activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Dense(4096, kernel_initializer='he_normal', activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Dense(num_class, kernel_initializer='he_normal', activation='softmax'))\n",
        "    return model\n",
        "\n",
        "model = create_model_conv((img_height,img_width,1))\n",
        "model.summary()#show模型結構\n",
        "x_train,x_test,y_train,y_test = get_dataset(save=True,load=False)\n",
        "lr = 0.0003\n",
        "import tensorflow as tf\n",
        "sgd = tf.keras.optimizers.Adam(lr=lr)\n",
        "run_opts = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom = True)\n",
        "model.compile(loss='binary_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    return lr * (0.1 ** int(epoch / 10))\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "history = model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test,y_test),verbose=1,callbacks=[LearningRateScheduler(lr_schedule),ModelCheckpoint('model.h5', save_best_only=True)])\n",
        "\n",
        "# 透過趨勢圖來觀察訓練與驗證的走向 (特別去觀察是否有\"過擬合(overfitting)\"的現象)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_train_history(history, train_metrics, val_metrics):\n",
        "    plt.plot(history.history.get(train_metrics),'-o')\n",
        "    plt.plot(history.history.get(val_metrics),'-o')\n",
        "    plt.ylabel(train_metrics)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['train', 'validation'])\n",
        "    \n",
        "    \n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plot_train_history(history, 'loss','val_loss')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plot_train_history(history, 'acc','val_acc')\n",
        "\n",
        "plt.show()\n",
        "y_pred = model.predict(x_test)\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_mat = confusion_matrix(np.where(y_test > 0)[1], np.argmax(y_pred, axis=1))\n",
        "classes = list(Chinese_data.values())\n",
        "df = pd.DataFrame(conf_mat, index=classes, columns=classes)\n",
        "\n",
        "fig = plt.figure(figsize = (10,10))\n",
        "sns.heatmap(df, annot=True, square=True, fmt='.0f', cmap=\"Blues\")\n",
        "plt.title('Simpson characters classification')\n",
        "plt.xlabel('ground_truth')\n",
        "plt.ylabel('prediction')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}